{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import pprint\n",
    "from time import sleep\n",
    "\n",
    "def flatten(l):\n",
    "    flatList = []\n",
    "    for elem in l:\n",
    "        # if an element of a list is a list\n",
    "        # iterate over this list and add elements to flatList \n",
    "        if type(elem) == list:\n",
    "            for e in elem:\n",
    "                flatList.append(e)\n",
    "        else:\n",
    "            flatList.append(elem)\n",
    "    return flatList\n",
    "\n",
    "def comment_crawler(url):\n",
    "    oid=url.split(\"oid=\")[1].split(\"&\")[0]\n",
    "    aid=url.split(\"aid=\")[1]\n",
    "    page=1  \n",
    "    header = {\n",
    "    \"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "    \"referer\":url,\n",
    "    } \n",
    "\n",
    "    List = []\n",
    "    while True :\n",
    "        sleep(0.01)\n",
    "        c_url=\"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback=jQuery1707138182064460843_1523512042464&lang=ko&country=&objectId=news\"+oid+\"%2C\"+aid+\"&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=\"+str(page)+\"&refresh=false&sort=FAVORITE\" \n",
    "        # 파싱하는 단계입니다.\n",
    "        r=requests.get(c_url,headers=header)\n",
    "        cont=BeautifulSoup(r.content,\"html.parser\")    \n",
    "        \n",
    "        total_comm=str(cont).split('comment\":')[1].split(\",\")[0]\n",
    "\n",
    "        match=re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(cont))\n",
    "        # 댓글을 리스트에 중첩합니다.\n",
    "        List.append(match)\n",
    "        # 한번에 댓글이 20개씩 보이기 때문에 한 페이지씩 몽땅 댓글을 긁어 옵니다.\n",
    "        if int(total_comm) <= ((page) * 20):\n",
    "            break\n",
    "        else : \n",
    "            page+=1\n",
    "    return flatten(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "read_path = '../article/'\n",
    "write_path = '../article_comment/'\n",
    "    \n",
    "for week in range(783):\n",
    "    print(f'>>> start {week} week article comment crawling')\n",
    "    article = pd.read_csv(f'{read_path}Article_부동산_{week}.csv', encoding = 'cp949')\n",
    "    df = pd.DataFrame({\n",
    "        'date' : article['date'],\n",
    "        'url' : article['url']\n",
    "    })\n",
    "    df['comment'] = [comment_crawler(url) for url in df['url']]\n",
    "    df.to_csv(f'{write_path}Article_부동산_{week}_댓글', encoding = 'utf-8')\n",
    "    print(f'>>> complete {week} week article comment crawling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_year': 2018, 'start_month': 1, 'end_year': 2018, 'end_month': 2}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8, euc-kr -*-\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import calendar\n",
    "import requests\n",
    "import re\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Process\n",
    "from korea_news_crawler.exceptions import *\n",
    "from korea_news_crawler.articleparser import ArticleParser\n",
    "from korea_news_crawler.writer import Writer\n",
    "\n",
    "class ArticleCrawler(object):\n",
    "    def __init__(self):\n",
    "        self.categories = {'부동산' : 260, '정치': 100, '경제': 101, '사회': 102, '생활문화': 103, '세계': 104, 'IT과학': 105, '오피니언': 110,\n",
    "                           'politics': 100, 'economy': 101, 'society': 102, 'living_culture': 103, 'world': 104, 'IT_science': 105, 'opinion': 110}\n",
    "        self.selected_categories = []\n",
    "        self.date = {'start_year': 0, 'start_month': 0, 'end_year': 0, 'end_month': 0}\n",
    "        self.user_operating_system = str(platform.system())\n",
    "\n",
    "    def set_category(self, *args):\n",
    "        for key in args:\n",
    "            if self.categories.get(key) is None:\n",
    "                raise InvalidCategory(key)\n",
    "        self.selected_categories = args\n",
    "\n",
    "    def set_date_range(self, start_year, start_month, end_year, end_month):\n",
    "        args = [start_year, start_month, end_year, end_month]\n",
    "        if start_year > end_year:\n",
    "            raise InvalidYear(start_year, end_year)\n",
    "        if start_month < 1 or start_month > 12:\n",
    "            raise InvalidMonth(start_month)\n",
    "        if end_month < 1 or end_month > 12:\n",
    "            raise InvalidMonth(end_month)\n",
    "        if start_year == end_year and start_month > end_month:\n",
    "            raise OverbalanceMonth(start_month, end_month)\n",
    "        for key, date in zip(self.date, args):\n",
    "            self.date[key] = date\n",
    "        print(self.date)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_news_page_url(category_url, start_year, end_year, start_month, end_month):\n",
    "        made_urls = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            target_start_month = start_month\n",
    "            target_end_month = end_month\n",
    "\n",
    "            if start_year != end_year:\n",
    "                if year == start_year:\n",
    "                    target_start_month = start_month\n",
    "                    target_end_month = 12\n",
    "                elif year == end_year:\n",
    "                    target_start_month = 1\n",
    "                    target_end_month = end_month\n",
    "                else:\n",
    "                    target_start_month = 1\n",
    "                    target_end_month = 12\n",
    "            \n",
    "            for month in range(target_start_month, target_end_month + 1):\n",
    "                for month_day in range(1, calendar.monthrange(year, month)[1] + 1):\n",
    "                    if len(str(month)) == 1:\n",
    "                        month = \"0\" + str(month)\n",
    "                    if len(str(month_day)) == 1:\n",
    "                        month_day = \"0\" + str(month_day)\n",
    "                        \n",
    "                    # 날짜별로 Page Url 생성\n",
    "                    url = category_url + str(year) + str(month) + str(month_day)\n",
    "\n",
    "                    # totalpage는 네이버 페이지 구조를 이용해서 page=10000으로 지정해 totalpage를 알아냄\n",
    "                    # page=10000을 입력할 경우 페이지가 존재하지 않기 때문에 page=totalpage로 이동 됨 (Redirect)\n",
    "                    totalpage = ArticleParser.find_news_totalpage(url + \"&page=10000\")\n",
    "                    for page in range(1, totalpage + 1):\n",
    "                        made_urls.append(url + \"&page=\" + str(page))\n",
    "        return made_urls\n",
    "\n",
    "    @staticmethod\n",
    "    def get_url_data(url, max_tries=5):\n",
    "        remaining_tries = int(max_tries)\n",
    "        while remaining_tries > 0:\n",
    "            try:\n",
    "                return requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "            except requests.exceptions:\n",
    "                sleep(1)\n",
    "            remaining_tries = remaining_tries - 1\n",
    "        raise ResponseTimeout()\n",
    "\n",
    "    def crawling(self, category_name):\n",
    "        # Multi Process PID\n",
    "        print(category_name + \" PID: \" + str(os.getpid()))    \n",
    "\n",
    "        writer = Writer(category='Article', article_category=category_name, date=self.date)\n",
    "        # 기사 url 형식\n",
    "        url_format = f'http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1={self.categories.get(category_name)}&date='\n",
    "        # start_year년 start_month월 ~ end_year의 end_month 날짜까지 기사를 수집합니다.\n",
    "        target_urls = self.make_news_page_url(url_format, self.date['start_year'], self.date['end_year'], self.date['start_month'], self.date['end_month'])\n",
    "\n",
    "        print(category_name + \" Urls are generated\")\n",
    "        print(\"The crawler starts\")\n",
    "\n",
    "        for url in target_urls:\n",
    "            request = self.get_url_data(url)\n",
    "            document = BeautifulSoup(request.content, 'html.parser')\n",
    "\n",
    "            # html - newsflash_body - type06_headline, type06\n",
    "            # 각 페이지에 있는 기사들 가져오기\n",
    "            temp_post = document.select('.newsflash_body .type06_headline li dl')\n",
    "            temp_post.extend(document.select('.newsflash_body .type06 li dl'))\n",
    "            \n",
    "            # 각 페이지에 있는 기사들의 url 저장\n",
    "            post_urls = []\n",
    "            for line in temp_post:\n",
    "                # 해당되는 page에서 모든 기사들의 URL을 post_urls 리스트에 넣음\n",
    "                post_urls.append(line.a.get('href'))\n",
    "            del temp_post\n",
    "\n",
    "            for content_url in post_urls:  # 기사 url\n",
    "                # 크롤링 대기 시간\n",
    "                sleep(0.01)\n",
    "                \n",
    "                # 기사 HTML 가져옴\n",
    "                request_content = self.get_url_data(content_url)\n",
    "\n",
    "                try:\n",
    "                    document_content = BeautifulSoup(request_content.content, 'html.parser')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # 기사 제목 가져옴\n",
    "                    tag_headline = document_content.find_all('h3', {'id': 'articleTitle'}, {'class': 'tts_head'})\n",
    "                    # 뉴스 기사 제목 초기화\n",
    "                    text_headline = ''\n",
    "                    text_headline = text_headline + ArticleParser.clear_headline(str(tag_headline[0].find_all(text=True)))\n",
    "                    # 공백일 경우 기사 제외 처리\n",
    "                    if not text_headline:\n",
    "                        continue\n",
    "\n",
    "                    # 기사 본문 가져옴\n",
    "                    tag_content = document_content.find_all('div', {'id': 'articleBodyContents'})\n",
    "                    # 뉴스 기사 본문 초기화\n",
    "                    text_sentence = ''\n",
    "                    text_sentence = text_sentence + ArticleParser.clear_content(str(tag_content[0].find_all(text=True)))\n",
    "                    # 공백일 경우 기사 제외 처리\n",
    "                    if not text_sentence:\n",
    "                        continue\n",
    "\n",
    "                    # 기사 언론사 가져옴\n",
    "                    tag_company = document_content.find_all('meta', {'property': 'me2:category1'})\n",
    "\n",
    "                    # 언론사 초기화\n",
    "                    text_company = ''\n",
    "                    text_company = text_company + str(tag_company[0].get('content'))\n",
    "\n",
    "                    # 공백일 경우 기사 제외 처리\n",
    "                    if not text_company:\n",
    "                        continue\n",
    "\n",
    "                    # 기사 시간대 가져옴\n",
    "                    time = re.findall('<span class=\"t11\">(.*)</span>',request_content.text)[0]\n",
    "\n",
    "                    # CSV 작성\n",
    "                    writer.write_row([time, category_name, text_company, text_headline, text_sentence, content_url])\n",
    "\n",
    "                    del time\n",
    "                    del text_company, text_sentence, text_headline\n",
    "                    del tag_company \n",
    "                    del tag_content, tag_headline\n",
    "                    del request_content, document_content\n",
    "\n",
    "                # UnicodeEncodeError\n",
    "                except Exception as ex:\n",
    "                    del request_content, document_content\n",
    "                    pass\n",
    "        writer.close()\n",
    "\n",
    "    def start(self):\n",
    "        # MultiProcess 크롤링 시작\n",
    "        for category_name in self.selected_categories:\n",
    "            proc = Process(target=self.crawling, args=(category_name,))\n",
    "            proc.start()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Crawler = ArticleCrawler()\n",
    "    Crawler.set_category('생활문화')\n",
    "    Crawler.set_date_range(2018, 1, 2018, 2)\n",
    "    Crawler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class ArticleParser(object):\n",
    "    special_symbol = re.compile('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$&▲▶◆◀■【】\\\\\\=\\(\\'\\\"]')\n",
    "    content_pattern = re.compile('본문 내용|TV플레이어| 동영상 뉴스|flash 오류를 우회하기 위한 함수 추가function  flash removeCallback|tt|앵커 멘트|xa0')\n",
    "\n",
    "    @classmethod\n",
    "    def clear_content(cls, text):\n",
    "        # 기사 본문에서 필요없는 특수문자 및 본문 양식 등을 다 지움\n",
    "        newline_symbol_removed_text = text.replace('\\\\n', '').replace('\\\\t', '').replace('\\\\r', '')\n",
    "        special_symbol_removed_content = re.sub(cls.special_symbol, ' ', newline_symbol_removed_text)\n",
    "        end_phrase_removed_content = re.sub(cls.content_pattern, '', special_symbol_removed_content)\n",
    "        blank_removed_content = re.sub(' +', ' ', end_phrase_removed_content).lstrip()  # 공백 에러 삭제\n",
    "        reversed_content = ''.join(reversed(blank_removed_content))  # 기사 내용을 reverse 한다.\n",
    "        content = ''\n",
    "        for i in range(0, len(blank_removed_content)):\n",
    "            # reverse 된 기사 내용중, \".다\"로 끝나는 경우 기사 내용이 끝난 것이기 때문에 기사 내용이 끝난 후의 광고, 기자 등의 정보는 다 지움\n",
    "            if reversed_content[i:i + 2] == '.다':\n",
    "                content = ''.join(reversed(reversed_content[i:]))\n",
    "                break\n",
    "        return content\n",
    "\n",
    "    @classmethod\n",
    "    def clear_headline(cls, text):\n",
    "        # 기사 제목에서 필요없는 특수문자들을 지움\n",
    "        newline_symbol_removed_text = text.replace('\\\\n', '').replace('\\\\t', '').replace('\\\\r', '')\n",
    "        special_symbol_removed_headline = re.sub(cls.special_symbol, '', newline_symbol_removed_text)\n",
    "        return special_symbol_removed_headline\n",
    "\n",
    "    @classmethod\n",
    "    def find_news_totalpage(cls, url):\n",
    "        # 당일 기사 목록 전체를 알아냄\n",
    "        try:\n",
    "            # Added headers for avoid anti-crawling\n",
    "            request_content = requests.get(url, timeout=10, headers={'User-Agent':'Mozilla/5.0'})\n",
    "            document_content = BeautifulSoup(request_content.content, 'html.parser')\n",
    "            headline_tag = document_content.find('div', {'class': 'paging'}).find('strong')\n",
    "            regex = re.compile(r'<strong>(?P<num>\\d+)')\n",
    "            match = regex.findall(str(headline_tag))\n",
    "            return int(match[0])\n",
    "        except Exception:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import csv\n",
    "import platform\n",
    "from korea_news_crawler.exceptions import *\n",
    "\n",
    "\n",
    "class Writer(object):\n",
    "    def __init__(self, category, article_category, date):\n",
    "        self.start_year = date['start_year']\n",
    "        self.start_month = f'0{date[\"start_month\"]}' if len(str(date['start_month'])) == 1 else str(date['start_month'])\n",
    "        self.end_year = date['end_year']\n",
    "        self.end_month = f'0{date[\"end_month\"]}' if len(str(date['end_month'])) == 1 else str(date['end_month'])\n",
    "\n",
    "        self.file = None\n",
    "        self.initialize_file(category, article_category)\n",
    "\n",
    "        self.csv_writer = csv.writer(self.file)\n",
    "\n",
    "    def initialize_file(self, category, article_category):\n",
    "        output_path = f'../output'\n",
    "        if os.path.exists(output_path) is not True:\n",
    "            os.mkdir(output_path)\n",
    "\n",
    "        file_name = f'{output_path}/{category}_{article_category}_{self.start_year}{self.start_month}_{self.end_year}{self.end_month}.csv'\n",
    "        if os.path.isfile(file_name):\n",
    "            raise ExistFile(file_name)\n",
    "\n",
    "        user_os = str(platform.system())\n",
    "        if user_os == \"Windows\":\n",
    "            self.file = open(file_name, 'w', encoding='euc-kr', newline='')\n",
    "        # Other OS uses utf-8\n",
    "        else:\n",
    "            self.file = open(file_name, 'w', encoding='utf-8', newline='')\n",
    "\n",
    "    def write_row(self, arg):\n",
    "        self.csv_writer.writerow(arg)\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 처리 가능한 값보다 큰 값이 나왔을 때\n",
    "class OverFlow(Exception):\n",
    "    def __init__(self, args):\n",
    "        self.message = f'{args} is overflow'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "    \n",
    "\n",
    "# 처리 가능한 값보다 작은 값이 나왔을 때\n",
    "class UnderFlow(Exception):\n",
    "    def __init__(self, args):\n",
    "        self.message = f'{args} is underflow'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "# 변수가 올바르지 않을 때\n",
    "class InvalidArgs(Exception):\n",
    "    def __init__(self, args):\n",
    "        self.message = f'{args} is Invalid Arguments'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "# 카테고리가 올바르지 않을 때\n",
    "class InvalidCategory(Exception):\n",
    "    def __init__(self, category):\n",
    "        self.message = f'{category} is Invalid Category.'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "# 년도가 올바르지 않을 때\n",
    "class InvalidYear(Exception):\n",
    "    def __init__(self, start_year, end_year):\n",
    "        self.message = f'{start_year}(start year) is bigger than {end_year}(end year)'\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.message)\n",
    "\n",
    "\n",
    "# 달이 올바르지 않을 때\n",
    "class InvalidMonth(Exception):\n",
    "    def __init__(self, month):\n",
    "        self.message = f'{month} is an invalid month'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "# 시작 달과 끝나는 달이 올바르지 않을 때\n",
    "class OverbalanceMonth(Exception):\n",
    "    def __init__(self, start_month, end_month):\n",
    "        self.message = f'{start_month}(start month) is an overbalance with {end_month}(end month)'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "# 실행시간이 너무 길어서 데이터를 얻을 수 없을 때\n",
    "class ResponseTimeout(Exception):\n",
    "    def __init__(self):\n",
    "        self.message = \"Couldn't get the data\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "# 존재하는 파일\n",
    "class ExistFile(Exception):\n",
    "    def __init__(self, path):\n",
    "        absolute_path = os.path.abspath(path)\n",
    "        self.message = f'{absolute_path} already exist'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_year': 2021, 'start_month': 1, 'end_year': 2021, 'end_month': 1}\n"
     ]
    }
   ],
   "source": [
    "Crawler = ArticleCrawler()  \n",
    "Crawler.set_category(\"부동산\")  \n",
    "Crawler.set_date_range(2021, 1, 2021, 1)  \n",
    "Crawler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
